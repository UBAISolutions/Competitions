{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5d0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Imports and configs\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from lightgbm import LGBMRegressor, early_stopping, log_evaluation\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from koolbox import Trainer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "import shutil\n",
    "import optuna\n",
    "import json\n",
    "import glob\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "class CFG:\n",
    "    train_path = \"/kaggle/input/playground-series-s5e5/train.csv\"\n",
    "    test_path = \"/kaggle/input/playground-series-s5e5/test.csv\"\n",
    "    sample_sub_path = \"/kaggle/input/playground-series-s5e5/sample_submission.csv\"\n",
    "    \n",
    "    original_path = \"/kaggle/input/calories-burnt-prediction/calories.csv\"\n",
    "\n",
    "    metric = root_mean_squared_error\n",
    "    target = \"Calories\"\n",
    "    n_folds = 5\n",
    "    seed = 42\n",
    "\n",
    "    cv = KFold(n_splits=n_folds, random_state=seed, shuffle=True)\n",
    "\n",
    "    run_optuna = True\n",
    "    n_optuna_trials = 250\n",
    "    \n",
    "##Data loading and preprocessing\n",
    "train = pd.read_csv(CFG.train_path, index_col=\"id\")\n",
    "test = pd.read_csv(CFG.test_path, index_col=\"id\")\n",
    "\n",
    "train[\"Sex\"] = train[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
    "test[\"Sex\"] = test[\"Sex\"].map({\"male\": 0, \"female\": 1})\n",
    "\n",
    "X = train.drop(CFG.target, axis=1)\n",
    "y = np.log1p(train[CFG.target])\n",
    "X_test = test\n",
    "original = pd.read_csv(CFG.original_path, index_col=\"User_ID\")\n",
    "original[\"Gender\"] = original[\"Gender\"].map({\"male\": 0, \"female\": 1})\n",
    "original = original.rename(columns={\"Gender\": \"Sex\"})\n",
    "\n",
    "X_original = original.drop(CFG.target, axis=1)\n",
    "y_original = np.log1p(original[CFG.target])\n",
    "mutual_info = mutual_info_regression(X, y, random_state=CFG.seed)\n",
    "\n",
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = X.columns\n",
    "mutual_info = pd.DataFrame(mutual_info.sort_values(ascending=False), columns=['Mutual Information'])\n",
    "mutual_info.style.bar(subset=['Mutual Information'], cmap='RdYlGn')\n",
    "\n",
    "mutual_info = mutual_info_regression(X_original, y_original, random_state=CFG.seed)\n",
    "\n",
    "mutual_info = pd.Series(mutual_info)\n",
    "mutual_info.index = X_original.columns\n",
    "mutual_info = pd.DataFrame(mutual_info.sort_values(ascending=False), columns=['Mutual Information'])\n",
    "mutual_info.style.bar(subset=['Mutual Information'], cmap='RdYlGn')\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "corr_train = train.corr()\n",
    "mask_train = np.triu(np.ones_like(corr_train, dtype=bool), k=1)\n",
    "sns.heatmap(\n",
    "    data=corr_train,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    mask=mask_train,\n",
    "    square=True,\n",
    "    cmap='coolwarm',\n",
    "    cbar_kws={'shrink': .7, 'format': '%.2f'},   \n",
    "    annot_kws={'size': 8},\n",
    "    center=0,\n",
    "    ax=axes[0]\n",
    ")\n",
    "axes[0].set_title('Train')\n",
    "axes[0].tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "corr_orig = original.corr()\n",
    "mask_orig = np.triu(np.ones_like(corr_orig, dtype=bool), k=1)\n",
    "sns.heatmap(\n",
    "    data=corr_orig,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    mask=mask_orig,\n",
    "    square=True,\n",
    "    cmap='coolwarm',\n",
    "    cbar_kws={'shrink': .7, 'format': '%.2f'},   \n",
    "    annot_kws={'size': 8},\n",
    "    center=0,\n",
    "    ax=axes[1]\n",
    ")\n",
    "axes[1].set_title('Original')\n",
    "axes[1].tick_params(axis='both', which='major', labelsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "##Training base models\n",
    "\n",
    "scores = {}\n",
    "oof_preds = {}\n",
    "test_preds = {}\n",
    "\n",
    "## HistGradientBoosting\n",
    "\n",
    "histgb_trainer = Trainer(\n",
    "    HistGradientBoostingRegressor(**histgb_params),\n",
    "    cv=CFG.cv,\n",
    "    metric=CFG.metric,\n",
    "    task=\"regression\"\n",
    ")\n",
    "\n",
    "histgb_trainer.fit(X, y, extra_X=X_original, extra_y=y_original)\n",
    "\n",
    "scores[\"HistGB\"] = histgb_trainer.fold_scores\n",
    "oof_preds[\"HistGB\"] = histgb_trainer.oof_preds\n",
    "test_preds[\"HistGB\"] = histgb_trainer.predict(X_test)\n",
    "\n",
    "##LightGBM (gbdt)\n",
    "\n",
    "lgbm_trainer = Trainer(\n",
    "    LGBMRegressor(**lgbm_params),\n",
    "    cv=CFG.cv,\n",
    "    metric=CFG.metric,\n",
    "    use_early_stopping=True,\n",
    "    task=\"regression\"\n",
    ")\n",
    "\n",
    "fit_args = {\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"callbacks\": [\n",
    "        log_evaluation(period=1000), \n",
    "        early_stopping(stopping_rounds=100)\n",
    "    ]\n",
    "}\n",
    "\n",
    "lgbm_trainer.fit(X, y, fit_args=fit_args, extra_X=X_original, extra_y=y_original)\n",
    "\n",
    "scores[\"LightGBM (gbdt)\"] = lgbm_trainer.fold_scores\n",
    "oof_preds[\"LightGBM (gbdt)\"] = lgbm_trainer.oof_preds\n",
    "test_preds[\"LightGBM (gbdt)\"] = lgbm_trainer.predict(X_test )\n",
    "\n",
    "##LightGBM (goss)\n",
    "\n",
    "lgbm_goss_trainer = Trainer(\n",
    "    LGBMRegressor(**lgbm_goss_params),\n",
    "    cv=CFG.cv,\n",
    "    metric=CFG.metric,\n",
    "    use_early_stopping=True,\n",
    "    task=\"regression\"\n",
    ")\n",
    "\n",
    "fit_args = {\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"callbacks\": [\n",
    "        log_evaluation(period=1000), \n",
    "        early_stopping(stopping_rounds=100)\n",
    "    ]\n",
    "}\n",
    "\n",
    "lgbm_goss_trainer.fit(X, y, fit_args=fit_args, extra_X=X_original, extra_y=y_original)\n",
    "\n",
    "scores[\"LightGBM (goss)\"] = lgbm_goss_trainer.fold_scores\n",
    "oof_preds[\"LightGBM (goss)\"] = lgbm_goss_trainer.oof_preds\n",
    "test_preds[\"LightGBM (goss)\"] = lgbm_goss_trainer.predict(X_test)\n",
    "\n",
    "##XGBoost\n",
    "\n",
    "xgb_trainer = Trainer(\n",
    "    XGBRegressor(**xgb_params),\n",
    "    cv=CFG.cv,\n",
    "    metric=CFG.metric,\n",
    "    use_early_stopping=True,\n",
    "    task=\"regression\"\n",
    ")\n",
    "\n",
    "fit_args = {\n",
    "    \"verbose\": 1000\n",
    "}\n",
    "\n",
    "xgb_trainer.fit(X, y, fit_args=fit_args, extra_X=X_original, extra_y=y_original)\n",
    "\n",
    "scores[\"XGBoost\"] = xgb_trainer.fold_scores\n",
    "oof_preds[\"XGBoost\"] = xgb_trainer.oof_preds\n",
    "test_preds[\"XGBoost\"] = xgb_trainer.predict(X_test)\n",
    "\n",
    "## CatBoost\n",
    "\n",
    "cb_trainer = Trainer(\n",
    "    CatBoostRegressor(**cb_params),\n",
    "    cv=CFG.cv,\n",
    "    metric=CFG.metric,\n",
    "    use_early_stopping=True,\n",
    "    task=\"regression\"\n",
    ")\n",
    "\n",
    "fit_args = {\n",
    "    \"verbose\": 1000,\n",
    "    \"early_stopping_rounds\": 100,\n",
    "    \"use_best_model\": True\n",
    "}\n",
    "\n",
    "cb_trainer.fit(X, y, fit_args=fit_args, extra_X=X_original, extra_y=y_original)\n",
    "\n",
    "scores[\"CatBoost\"] = cb_trainer.fold_scores\n",
    "oof_preds[\"CatBoost\"] = cb_trainer.oof_preds\n",
    "test_preds[\"CatBoost\"] = cb_trainer.predict(X_test)\n",
    "\n",
    "\n",
    "## AutoGluon\n",
    "\n",
    "oof_preds_files = glob.glob(f'/kaggle/input/s05e05-calorie-expenditure-prediction-automl/*_oof_preds_*.pkl')\n",
    "test_preds_files = glob.glob(f'/kaggle/input/s05e05-calorie-expenditure-prediction-automl/*_test_preds_*.pkl')\n",
    "\n",
    "ag_oof_preds = np.log1p(joblib.load(oof_preds_files[0]))\n",
    "ag_test_preds = np.log1p(joblib.load(test_preds_files[0]))\n",
    "\n",
    "ag_scores = []\n",
    "split = KFold(n_splits=CFG.n_folds, random_state=CFG.seed, shuffle=True).split(X, y)\n",
    "for _, val_idx in split:\n",
    "    y_val = y[val_idx]\n",
    "    y_preds = ag_oof_preds[val_idx]   \n",
    "    score = root_mean_squared_error(y_preds, y_val)\n",
    "    ag_scores.append(score)\n",
    "    \n",
    "oof_preds[\"AutoGluon\"], test_preds[\"AutoGluon\"], scores[\"AutoGluon\"] = ag_oof_preds, ag_test_preds, ag_scores\n",
    "\n",
    "## Ensembling with Ridge\n",
    "\n",
    "X = pd.DataFrame(oof_preds)\n",
    "X_test = pd.DataFrame(test_preds)\n",
    "\n",
    "joblib.dump(X, \"oof_preds.pkl\")\n",
    "joblib.dump(X_test, \"test_preds.pkl\")\n",
    "\n",
    "def objective(trial):    \n",
    "    params = {\n",
    "        \"random_state\": CFG.seed,\n",
    "        \"alpha\": trial.suggest_float(\"alpha\", 0, 10),\n",
    "        \"tol\": trial.suggest_float(\"tol\", 1e-6, 1e-2)\n",
    "    }\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        Ridge(**params),\n",
    "        cv=CFG.cv,\n",
    "        metric=CFG.metric,\n",
    "        task=\"regression\",\n",
    "        verbose=False\n",
    "    )\n",
    "    trainer.fit(X, y)\n",
    "    \n",
    "    return np.mean(trainer.fold_scores)\n",
    "\n",
    "if CFG.run_optuna:\n",
    "    sampler = optuna.samplers.TPESampler(seed=CFG.seed, multivariate=True)\n",
    "    study = optuna.create_study(direction=\"minimize\", sampler=sampler)\n",
    "    study.optimize(objective, n_trials=CFG.n_optuna_trials, n_jobs=-1, catch=(ValueError,))\n",
    "    best_params = study.best_params\n",
    "\n",
    "    ridge_params = {\n",
    "        \"random_state\": CFG.seed,\n",
    "        \"alpha\": best_params[\"alpha\"],\n",
    "        \"tol\": best_params[\"tol\"]\n",
    "    }\n",
    "else:\n",
    "    ridge_params = {\n",
    "        \"random_state\": CFG.seed\n",
    "    }\n",
    "    \n",
    "print(json.dumps(ridge_params, indent=2))    \n",
    "\n",
    "ridge_trainer = Trainer(\n",
    "    Ridge(**ridge_params),\n",
    "    cv=CFG.cv,\n",
    "    metric=CFG.metric,\n",
    "    task=\"regression\"\n",
    ")\n",
    "\n",
    "ridge_trainer.fit(X, y)\n",
    "\n",
    "scores[\"Ridge (ensemble)\"] = ridge_trainer.fold_scores\n",
    "ridge_test_preds = np.expm1(ridge_trainer.predict(X_test))\n",
    "\n",
    "ridge_coeffs = np.zeros((1, X.shape[1]))\n",
    "for m in ridge_trainer.estimators:\n",
    "    ridge_coeffs += m.coef_\n",
    "ridge_coeffs = ridge_coeffs / len(ridge_trainer.estimators)\n",
    "\n",
    "plot_weights(ridge_coeffs, \"Ridge Coefficients\")\n",
    "\n",
    "\n",
    "## Submission\n",
    "sub = pd.read_csv(CFG.sample_sub_path)\n",
    "sub[CFG.target] = ridge_test_preds\n",
    "sub.to_csv(f\"sub_ridge_{np.mean(scores['Ridge (ensemble)']):.6f}.csv\", index=False)\n",
    "sub.head()\n",
    "\n",
    "## Results\n",
    "scores = pd.DataFrame(scores)\n",
    "mean_scores = scores.mean().sort_values(ascending=True)\n",
    "order = scores.mean().sort_values(ascending=True).index.tolist()\n",
    "\n",
    "min_score = mean_scores.min()\n",
    "max_score = mean_scores.max()\n",
    "padding = (max_score - min_score) * 0.5\n",
    "lower_limit = min_score - padding\n",
    "upper_limit = max_score + padding\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(15, scores.shape[1] * 0.3))\n",
    "\n",
    "boxplot = sns.boxplot(data=scores, order=order, ax=axs[0], orient=\"h\", color=\"grey\")\n",
    "axs[0].set_title(f\"Fold {CFG.metric.__name__}\")\n",
    "axs[0].set_xlabel(\"\")\n",
    "axs[0].set_ylabel(\"\")\n",
    "\n",
    "barplot = sns.barplot(x=mean_scores.values, y=mean_scores.index, ax=axs[1], color=\"grey\")\n",
    "axs[1].set_title(f\"Average {CFG.metric.__name__}\")\n",
    "axs[1].set_xlabel(\"\")\n",
    "axs[1].set_xlim(left=lower_limit, right=upper_limit)\n",
    "axs[1].set_ylabel(\"\")\n",
    "\n",
    "for i, (score, model) in enumerate(zip(mean_scores.values, mean_scores.index)):\n",
    "    color = \"cyan\" if \"ensemble\" in model.lower() else \"grey\"\n",
    "    barplot.patches[i].set_facecolor(color)\n",
    "    boxplot.patches[i].set_facecolor(color)\n",
    "    barplot.text(score, i, round(score, 6), va=\"center\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "shutil.rmtree(\"catboost_info\", ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d7ec73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
